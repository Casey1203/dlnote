参数初始化

深度学习模型的训练算法需要靠迭代，要求使用者指定一些开始迭代的初始点。有些初始点十分不稳定，使得算法遇到数值问题，导致完全失败。初始点可以决定收敛速度，以及收敛到的损失值的大小，甚至是影响泛化误差。

初始参数需要在不同单元间“破坏对称性”。”对称性“的意思是，将权重初始化为一样的数。这会导致这些单元的更新方式彼此相同，从而降低了网络的随机性，导致在前向传播中丢失了一些输入的模式，以及在反向传播中的一些梯度的模式。因此一般采用随机初始化权重矩阵。而每个单元的偏置，则设置为常数。

更大的初始权重更有利于“破坏对称性”，因为在矩阵乘法中，有更大的输出，从而避免前向和反向传播中丢失信号。但是，如果初始化权重太大，会导致梯度爆炸，或者激活函数饱和，导致梯度消失。

Understanding the difficulty of training deep feedforward neural networks(2010), Xavier et al这篇论文提出了一个权重初始化的方法，称为Xavier initialization。
$$
\mathrm{W}_{i, j} \sim U\left(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right)
$$
这篇论文的目的是，使每一层输出的方差尽可能相等。可以简单推导一下如何做到的。

假设有两个随机变量$x$和$w$，它们服从均值为$0$，方差分别为$\sigma_x$和$\sigma_w$的分布，且独立同分布，有以下性质

1. $w\cdot x$就会服从均值为$0$，方差为$\sigma_x\sigma_w$的分布
2. $w\cdot x + w \cdot x$就会服从均值为$0$，方差为$2\sigma_x\sigma_w$的分布。

在全连接网络前向传播中，$x$是输入层，$w$是输入层和隐藏层之间的权重，那么隐藏层中第$j$个神经元的输入为
$$
z_j^{(2)}=\sum_{i}^{n^{(2)}} w_{ji}^{(1)} \cdot x_i
$$
这里忽略了偏置项$b_j^{(2)}$，上标$(1)$和$(2)$分别表示第1层和第2层。

假设$w$和$x$独立同分布，则$z_j^{(2)}$的方差为$\sigma_z^{(2)}=n^{(2)}\sigma_w^{(1)}\sigma_x^{(1)}=n^{(2)}\sigma_w^{(1)}\sigma_a^{(1)}$。

假设没有激活函数，则$z_j^{(2)}=a_j^{(2)}$。

继续计算第3层第$j$个神经元的输入
$$
z_j^{(3)}=\sum_{i}^{n^{(3)}}w_{ji}^{(2)}\cdot a_i^{(2)}
$$
则$z_j^{(3)}$的方差为$\sigma_z^{(3)}=n^{(3)}\sigma_w^{(2)}\sigma_a^{(2)}=n^{(3)}\sigma_w^{(2)}n^{(2)}\sigma_w^{(1)}\sigma_a^{(1)}$

假设有$K$层网络，则第$K$层的输入$z^{(K)}$​的方差为
$$
\sigma_z^{(K)}=\sigma_a^{(1)}\prod_{i=1}^{K-1} n^{(i+1)}\sigma_w^{(i)}
$$
可以看到，如果$n^{(i+1)}\sigma_w^{(i)}$大于1，则$\prod_i^{K-1}$会随着网络层数越深，方差越大。反之，随着网络层数越深，方差越小。

回到论文的主题，想要让两层的输出方差尽可能相等，即$\sigma_a^{(k+1)}=\sigma_a^{(k)}$，则$\sigma_w^{(k)}=\frac{1}{n^{(k+1)}}$。

反向传播中，假设损失关于第$k$层输入的梯度是$\nabla_{a^{(k)}}L$，则损失关于第$k-1$层的梯度是
$$
\nabla_{{a^{(k-1)}_j}}L=\sum_{i=1}^{n^{(k)}}\nabla_{{a^{(k)}_i}}w^{(k)}_{ij}
$$
因此，第$k-1$层反向传播的梯度的方差
$$
\sigma_{\nabla_{a^{(k-1)}}L}=n^{(k)}\sigma_{\nabla_{a^{(k)}}L}\sigma_w^{(k)}
$$
一直反向传播到第1层，方差的公式可以写作
$$
\sigma_{\nabla_{a^{(1)}}L}=\sigma_{\nabla_{a^{(K)}}L}\prod_{i=1}^{K-1}n^{(i)}\sigma_w^{(k)}
$$
为了使$\sigma_{\nabla_{a^{(k-1)}}L}=\sigma_{\nabla_{a^{(k)}}L}$，推出$\sigma_w^{(k)}=\frac{1}{n^{(k)}}$。

结合前向和反向传播，得到$\sigma_w^{(k)}=\frac{1}{n^{(k+1)}}$和$\sigma_w^{(k)}=\frac{1}{n^{(k)}}$。

因此考虑到均衡，$\sigma_w^{(k)}=\frac{2}{n^{(k)}+n^{(k+1)}}$。

论文中利用均匀分布进行初始化，假设范围是$[-a,a]$，而均匀分布的方差是
$$
\sigma_{uniform}=\frac{(a-(-a))^2}{12}=\frac{a^2}{3}=\sigma_w^{(k)}=\frac{2}{n^{(k)}+n^{(k+1)}}
$$
推出
$$
a=\sqrt{\frac{6}{n^{(k)}+n^{(k+1)}}}
$$




