随机梯度下降

随机梯度下降(SGD)是机器学习问题应用最多的优化算法，步骤如下

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/dlnote/master/ch8/img/algo8.1.png" width = 90% height = 90% div/>
</figure>

其中$\frac{1}{m}\nabla_iL(f(x^{(i)};\theta),y^{(i)})$表示损失函数$L$在每个样本的地方关于参数$\theta$的梯度的平均值。然后用梯度的平均值去更新参数$\theta$。过程中，最重要的参数是学习率$\epsilon$，实践中学习率随时间推移而衰减，因此定义第$k$步迭代的学习率为$\epsilon_k$，衰减过程为$\epsilon_k=(1-\alpha)\epsilon_0 + \alpha \epsilon_{\tau}$，其中$\alpha=\frac{k}{\tau}$。一般当$k=\tau$时，$\alpha=1$，此时$\epsilon_k=\epsilon_{\tau}$，之后就保持学习率不变。

这是一个将学习率线性衰减的过程，引入了超参数$\tau$和$\epsilon_0$。通常$\epsilon_{\tau}$约为$\epsilon_0$的$1\%$，表示$\tau$应设为迭代几百次。而初始学习率$\epsilon_0$的设置方法为，从总训练时间和最终的损失函数值而言，初始学习率$\epsilon_0$一般设置为比大约迭代100次左右达到最佳效果的学习率稍微再高一点。

动量

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/dlnote/master/ch8/img/fig8.5.png" width = 40% height = 40% div/>
</figure>

等高线描绘了一个二次损失函数，图中红色路径表示动量学习规则走出来的路径。在该路径的每一步画了一个箭头，表示梯度下降在该点采取的步骤。可以看到普通的梯度下降相比于动量方法，会在通向最小值的路径两边左右晃动，而动量方法则比较正确的直接穿过路径直达最小值。

动量算法的过程如下

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/dlnote/master/ch8/img/algo8.2.png" width = 90% height = 90% div/>
</figure>

动量算法借鉴了牛顿运动定律，引入了速度$v$，这是一个向量，表示在参数空间的移动方向和速率。根据动量定律，物体的动量等于物体的速度乘以质量，假设物体为单位质量，则此时物体的动量就等于速度。

在每次迭代中，会计算损失函数在每个样本的地方关于参数$\theta$的梯度的平均值$g$，然后会用$g$去更新速度$v$，由$\epsilon$和$\alpha$来控制更新过程。这表明速度$v$累积了迭代过程中所有的梯度，当$\alpha$越大，表明之前的梯度对现在方向的影响也很大。实践中，$\alpha$一般取$0.5$，$0.9$和$0.99$，并且也会随着时间不断调整，从小变大。

Nesterov动量

Nesterov动量算法是标准的动量算法的一个变种，过程为

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/dlnote/master/ch8/img/algo8.3.png" width = 90% height = 90% div/>
</figure>

与标准动量算法唯一的区别在于梯度$g$的计算上，Nesterov动量算法在计算梯度时，首先会将参数$\theta$临时更新为$\tilde{\theta}=\theta+\alpha v$，然后将损失函数$L$关于$\tilde{\theta}$求梯度$g$，利用该梯度去更新速度。